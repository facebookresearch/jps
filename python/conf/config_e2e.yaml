method : "iql"

# game settings
seed : 10001
# discount factor
train_bomb : 0
eval_bomb : 0
greedy_extra : 0

# optimization/training settings
# Learning rate
lr : 6.25e-5
# Adam epsilon
eps : 1.5e-4
# max grad norm
grad_clip : 50

train_device : "cuda:0"
batchsize : 1024
num_epoch : 5000
epoch_len : 10000
num_update_between_sync : 2500

# replay buffer settings
burn_in_frames : 80000
# [NOTE] Replay size should be relatively small, otherwise it has a lot old pairs played by old models. 
replay_buffer_size : 800000

# prioritized replay alpha
priority_exponent : 0.6
# prioritized replay beta
priority_weight : 0.4

# prefetch batch
prefetch : 3

# thread setting
## #thread_loop
num_thread : 40
num_game_per_thread : 1

# actor setting
act_device : "cuda:1"

# others
record_time : 0
eval_only : false
eval_fake_epoch: -1
eval_first_k: -1

save_prefix: ""
display_freq: 20000

githash: ""
sweep_filename: ""

negative_momentum:

defaults:
    - game: bridge
    - method: a2c_e2e
    - baseline: a2c_e2e
    - env_actor_gen: env_actor_gen
    - trainer: selfplay
